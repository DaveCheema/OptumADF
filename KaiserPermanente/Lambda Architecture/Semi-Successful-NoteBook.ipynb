{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure\n{ \"name\":\"Spark-to-Cosmos_DB_Change_Feed_Connector\", \n  \"executorMemory\": \"8G\", \n  \"executorCores\": 2, \n  \"numExecutors\": 2, \n  \"driverCores\": 2,\n  \"jars\": [\"wasb:///example/jars/azure-cosmosdb-spark_2.4.0_2.11-1.4.1.jar\", \"wasb:///example/jars/azure-documentdb-1.13.0.jar\", \"wasb:///example/jars/azure-documentdb-rx-0.9.0-rc2.jar\", \"wasb:///example/jars/json-20140107.jar\", \"wasb:///example/jars/rxjava-1.3.0.jar\", \"wasb:///example/jars/rxnetty-0.4.20.jar\"],  \n  \"conf\": {\n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'pyspark', u'name': u'Spark-to-Cosmos_DB_Change_Feed_Connector', u'numExecutors': 2, u'conf': {u'spark.jars.excludes': u'org.scala-lang:scala-reflect'}, u'executorCores': 2, u'driverCores': 2, u'jars': [u'wasb:///example/jars/azure-cosmosdb-spark_2.4.0_2.11-1.4.1.jar', u'wasb:///example/jars/azure-documentdb-1.13.0.jar', u'wasb:///example/jars/azure-documentdb-rx-0.9.0-rc2.jar', u'wasb:///example/jars/json-20140107.jar', u'wasb:///example/jars/rxjava-1.3.0.jar', u'wasb:///example/jars/rxnetty-0.4.20.jar'], u'executorMemory': u'8G'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 26.201904296875, "end_time": 1576200807452.495}}, "collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "database = \"dc1-test-db\"\ncollection = \"states\"\n\ntweetsConfig = {\n\"Endpoint\" : \"https://dc1-cosmos-db.documents.azure.com:443/\",\n\"Masterkey\" : \"48pZMByEhd5DNtylKgMK2n34AcAF0VTXsP0ewaTfJw1LMBBHaCG6AxcpZOgoeZT7VhNHU7Rs7dhjkvVzHbkFnQ==\",\n\"Database\" : database,\n\"Collection\" : collection, \n\"preferredRegions\" : \"West US\",\n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"200000\",\n\"query_custom\" : \"select * FROM c\"\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>10</td><td>application_1576190073047_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-dc1hdi.0t04vm2df5ve1a0cf2pxuar1sd.dx.internal.cloudapp.net:8088/proxy/application_1576190073047_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-dc1hdi.0t04vm2df5ve1a0cf2pxuar1sd.dx.internal.cloudapp.net:30060/node/containerlogs/container_1576190073047_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 63.4658203125, "end_time": 1576200858679.001}}, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "tweets = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**tweetsConfig).load()", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling o97.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, wn1-dc1hdi.0t04vm2df5ve1a0cf2pxuar1sd.dx.internal.cloudapp.net, executor 2): java.lang.NoSuchMethodError: com.microsoft.azure.documentdb.FeedOptions.setResponseContinuationTokenLimitInKb(I)V\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.queryDocuments$1(CosmosDBRDDIterator.scala:195)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader$lzycompute(CosmosDBRDDIterator.scala:423)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader(CosmosDBRDDIterator.scala:146)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.hasNext(CosmosDBRDDIterator.scala:446)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBSchema.schema(CosmosDBSchema.scala:71)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.com$microsoft$azure$cosmosdb$spark$schema$CosmosDBRelation$$lazySchema$lzycompute(CosmosDBRelation.scala:59)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.com$microsoft$azure$cosmosdb$spark$schema$CosmosDBRelation$$lazySchema(CosmosDBRelation.scala:44)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation$$anonfun$schema$1.apply(CosmosDBRelation.scala:62)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation$$anonfun$schema$1.apply(CosmosDBRelation.scala:62)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.schema$lzycompute(CosmosDBRelation.scala:62)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.schema(CosmosDBRelation.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoSuchMethodError: com.microsoft.azure.documentdb.FeedOptions.setResponseContinuationTokenLimitInKb(I)V\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.queryDocuments$1(CosmosDBRDDIterator.scala:195)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader$lzycompute(CosmosDBRDDIterator.scala:423)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader(CosmosDBRDDIterator.scala:146)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.hasNext(CosmosDBRDDIterator.scala:446)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\", line 172, in load\n    return self._df(self._jreader.load())\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o97.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, wn1-dc1hdi.0t04vm2df5ve1a0cf2pxuar1sd.dx.internal.cloudapp.net, executor 2): java.lang.NoSuchMethodError: com.microsoft.azure.documentdb.FeedOptions.setResponseContinuationTokenLimitInKb(I)V\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.queryDocuments$1(CosmosDBRDDIterator.scala:195)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader$lzycompute(CosmosDBRDDIterator.scala:423)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader(CosmosDBRDDIterator.scala:146)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.hasNext(CosmosDBRDDIterator.scala:446)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBSchema.schema(CosmosDBSchema.scala:71)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.com$microsoft$azure$cosmosdb$spark$schema$CosmosDBRelation$$lazySchema$lzycompute(CosmosDBRelation.scala:59)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.com$microsoft$azure$cosmosdb$spark$schema$CosmosDBRelation$$lazySchema(CosmosDBRelation.scala:44)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation$$anonfun$schema$1.apply(CosmosDBRelation.scala:62)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation$$anonfun$schema$1.apply(CosmosDBRelation.scala:62)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.schema$lzycompute(CosmosDBRelation.scala:62)\n\tat com.microsoft.azure.cosmosdb.spark.schema.CosmosDBRelation.schema(CosmosDBRelation.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:432)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoSuchMethodError: com.microsoft.azure.documentdb.FeedOptions.setResponseContinuationTokenLimitInKb(I)V\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.queryDocuments$1(CosmosDBRDDIterator.scala:195)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader$lzycompute(CosmosDBRDDIterator.scala:423)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.reader(CosmosDBRDDIterator.scala:146)\n\tat com.microsoft.azure.cosmosdb.spark.rdd.CosmosDBRDDIterator.hasNext(CosmosDBRDDIterator.scala:446)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 9374.968994140625, "end_time": 1576200880843.344}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}